# çˆ¬èŸ²æŠ“å–pttç†±é–€æ–‡ç« 
# è§£æ±ºå•é¡Œ
1. ä»¥å¾€è¦æ‰¾å–ç†±é–€çš„æ–‡ç« ï¼Œåªèƒ½ä¸€é ä¸€é æ‰¾ï¼Œä¸æ–¹ä¾¿ä¸”è€—æ™‚ï¼Œä½¿ç”¨é€™å€‹è»Ÿé«”ï¼Œå¯ä»¥å¿«é€ŸæŒæ¡ç•¶å¤©è¨è«–åº¦æœ€é«˜çš„æ–‡ç« ï¼Œç¯€çœéå¸¸å¤šçš„æ™‚é–“ã€‚

# ä½œå“ä»‹ç´¹

1. åŸ·è¡Œä¸»ç¨‹å¼ä½¿ç”¨çˆ¬èŸ²çˆ¬å–pptç†±é–€æ–‡ç« å¾Œï¼Œæœƒç”¢ç”Ÿä¸€å€‹csvæª”æ¡ˆã€‚
2. æ­¤csvæª”æ¡ˆï¼ŒæœƒæŠ“å–æ¨æ–‡æ•¸æœ€é«˜çš„å‰Nåæ–‡ç« (ä»£è¡¨æœ€ç†±é–€çš„å‰Nåæ–‡ç« )ï¼Œä¸¦ä¸”åŒæ™‚æŠ“å–æ¨æ–‡æ•¸ï¼Œæ¨™é¡Œï¼Œæ–‡ç« é€£çµï¼Œä½œè€…ï¼Œå…§æ–‡ï¼Œå’Œç™¼ä½ˆæ™‚é–“ã€‚
4. æˆ‘å€‘çš„æœå°‹ç¯„åœç‚ºé æ•¸ï¼Œå¯åœ¨setting.py æª”æ¡ˆä¸­åšèª¿æ•´ï¼Œä¹Ÿå¯ä¿®æ”¹ç¯©é¸å‰100åæˆ–æ˜¯å‰10åçš„æ–‡ç« ã€‚
5. ä½¿ç”¨è€…å¯ä»¥é»é¸æ–‡ç« é€£çµç›´æ¥é€£åˆ°ç¶²ç«™ã€‚

![](https://i.imgur.com/opnMNqt.png)



# æ“ä½œèªªæ˜ğŸ› 

ä¸»è¦æª”æ¡ˆç‚ºå…©å€‹
1. main.py ç‚ºä¸»ç¨‹å¼ï¼Œæˆ‘å€‘éœ€è¦å»åŸ·è¡Œä»–ï¼Œä¸‹è¼‰å¾Œåœ¨åŒå€‹è³‡æ–™å¤¾è·¯å¾‘ä¸‹ï¼Œé–‹èµ·çµ‚ç«¯æ©Ÿï¼Œç„¶å¾Œè¼¸å…¥
`python main.py` åŸ·è¡Œå¾Œå³å¯é€²è¡Œçˆ¬èŸ²
2. setting.py ç‚ºæˆ‘å€‘åƒæ•¸å€¼çš„è¨­å®šç”¨çš„æª”æ¡ˆã€‚
3. PTT\_tool\_env.yml é€™å€‹æª”æ¡ˆæ˜¯é€™å€‹ç¨‹å¼çš„åŸ·è¡Œç’°å¢ƒï¼Œå¤§å®¶å¯ä»¥ä¸‹è¼‰å¾Œæ”¾åˆ°anacondaï¼Œè®“å¾Œå†å»åŸ·è¡Œä»–ã€‚è‡³æ–¼è¦æ€éº¼æ”¾ï¼Œå¤§å®¶å¯ä»¥åƒè€ƒé€™ç¯‡æ–‡ç« ï¼Œæˆ–è€…æ˜¯ç›´æ¥æ‰‹å‹•import å› ç‚ºç›¸é—œçš„ç‰¹åˆ¥å¥—ä»¶ä¹Ÿä¸å¤šï¼Œæ‰€ä»¥å› è©²ä¸æœƒå¤ªéº»ç…©ã€‚
https://medium.com/qiubingcheng/%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%9Danaconda-%E4%B8%A6%E4%B8%94%E5%8C%AF%E5%85%A5conda%E8%99%9B%E6%93%AC%E7%92%B0%E5%A2%83-ba2e140706a3

![](https://i.imgur.com/qpXYWpw.png)


# setting.pyåƒæ•¸æª”æ¡ˆè¨­å®š
æ‰“é–‹æª”æ¡ˆå¾Œï¼Œå°å¼Ÿæœ‰æŠŠç›¸é—œåƒæ•¸çš„è¨»è§£ä¸‹ä¸‹ä¾† ï¼Œ
ç›¸é—œåƒæ•¸åŠŸèƒ½ï¼Œå¯ä»¥åƒè€ƒè¨»è§£ä¸Šçš„å…§å®¹ï¼Œç„¶å¾Œé€™å€‹setting.pyæª”æ¡ˆæœƒè¢«main.py ä¸»ç¨‹å¼å¼•å…¥ã€‚
```python=
url="https://www.ptt.cc/bbs/studyabroad/index.html"  # ä½ è¦çˆ¬å–çš„ç¶²ç«™çš„ç¶²å€ï¼Œé è¨­ç‚ºptt ç•™å­¸ç‰ˆ


page=5 # ç‚ºä½ è¦å¾€å¾Œçˆ¬å–çš„é æ•¸

sort_number=10 # ç‚ºä½ è¦å–å‡ºçš„pushæ•¸æœ€å¤šçš„å‰å¹¾ç¯‡æ–‡ç« ï¼Œé è¨­ç‚ºå‰10å

file_name="result.csv" #ç‚ºç”¢ç”Ÿçš„csvæª”æ¡ˆè·¯å¾‘ä»¥åŠåç¨±ï¼Œé è¨­ç‚ºåœ¨åŒå€‹è³‡æ–™å¤¾

```



# ç¨‹å¼ç¢¼èªªæ˜ 

### payload éƒ¨åˆ†
1. é€™é‚Šä½¿ç”¨payload å»å‘serverå‚³é€è«‹æ±‚ ä¸¦ä¸”ä½¿ç”¨headers è®“æˆ‘å€‘åœ¨æŠ“å–è³‡æ–™çš„æ™‚å€™ä¸è¦è¢«ç³»çµ±ç™¼ç¾æ˜¯ç”¨ç¨‹å¼å»æŠ“çš„

2. å¯ä»¥çœ‹åˆ°è¡¨å–®æ˜¯ä»¥POSTçš„å½¢å¼å‚³é€ï¼Œç¢ºèªé è¨­çš„å€¼æ˜¯'yes'ï¼Œ
æ‰€ä»¥æ¥ä¸‹ä¾†æˆ‘å€‘è¦å¸¶è‘—å»ºç«‹çš„sessionï¼Œä»¥POSTçš„æ–¹å¼å¸¶è‘—åƒæ•¸ç™»å…¥ï¼Œå†ç”¨cookieä»¥GETçš„æ–¹å¼å¸¶è‘—åƒæ•¸é€²å…¥ä¸»é ã€‚

``` python=
payload = {"from": "https://www.ptt.cc/bbs/Gossiping/index.html","yes": "yes"} 
my_headers = {"user-agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36"}

rs = requests.Session() 
rs.post("https://www.ptt.cc/", data = payload, headers = my_headers) 
```

### è’é›†å‰né æ–‡ç« æ–¹æ³•&å¾€å‰ä¸€é çˆ¬èŸ²æ–¹æ³• 
1. é¦–å…ˆï¼Œé€™é‚Šå…ˆå»ä½¿ç”¨bs4.BeautifulSoupè§£æäº†urlé€™å€‹ç¶²å€(é€™å€‹ç¶²å€æ˜¯ç¬¬ä¸€é )ã€‚ 
2. é‚£å†ä¾†æœƒå»æŠŠé€™å€‹ç¶²é æ¨™ç±¤titleä¸‹çš„é€£çµ(æ–‡ç« é€£çµ)åŠ å…¥linké€™å€‹listã€‚ 
3. ç„¶å¾Œå†å»ä½¿ç”¨soup.select()æŠŠä¸»ç•«é¢ä¸­çš„ä¸Šä¸€é æŒ‰éˆ•çš„é€£çµæŠ“ä¸‹ä¾†ï¼Œå›å‚³å›å»ï¼Œé€™æ¨£ä¸‹ä¸€æ¬¡åšfor loop å°±å¯ä»¥æŠŠé€™å€‹é€£çµç•¶ä½œæ–°çš„url ï¼Œé€™æ¨£å°±å®Œæˆäº†å¾€å‰ä¸€é çˆ¬èŸ²çš„å‹•ä½œã€‚ 

```python=
def get_date(url): 
    response = rs.get(url, headers=my_headers)
    soup = bs4.BeautifulSoup(response.text, "html.parser")           #é€éBeautifulSoupä¸¦ç”¨"html.parser"å»è§£ææ­¤ç¶²ç«™
    titles = soup.find_all("div", class_ = "title")          #æŠ“å–æ¯ç¯‡æ–‡ç« é€£çµ
    #print(titles)
    dates = soup.find_all("div", class_="date")       #æŠ“å–æ—¥æœŸ
    #print(dates)
    print(" çˆ¬èŸ²é€²è¡Œä¸­....")
    for i in range(len(titles)):                         #ç”¨titleç•¶æ–‡ç« æ•¸é‡çš„è¨ˆæ•¸å™¨ å»ä¸€ä¸€æª¢è¦–æ—¥æœŸ 
            if(titles[i].a):                           #ä½¿ç”¨.aæ˜¯é¿å…å·²ç¶“è¢«åˆªé™¤çš„æ–‡ç« 
                #print(i)
                # æŠŠæ‰€æœ‰æ–‡ç« çš„é€£çµéƒ½åŠ å…¥link 
                link.append("https://www.ptt.cc/" + titles[i].find("a").get("href")) # å¾—åˆ°aæ¨™ç±¤ä¸‹çš„href  
                
    nextlink_2=soup.select('div.btn-group > a')           # ç”±ç±¤div.btn.group åˆ°æ¨™ç±¤a 
    up_page_href = nextlink_2[3]['href'] #åœ¨æ¨™ç±¤div.btn.groupä¸‹[0]æ˜¯çœ‹æ¿é€£çµ  [1]æ˜¯ç²¾è¯å€é€£çµ [2]æ˜¯æœ€èˆŠ [3]æ˜¯ä¸Šé  [4]æ˜¯ä¸‹é  [5]æ˜¯æœ€æ–°
    return  up_page_href  #å›å‚³ä¸Šä¸€é çš„ç¶²å€ è®“ä¸‹é¢çš„for loop éè¿´æˆåŠŸ
```

###  äº†è§£åŸå§‹ç¶²é ä¸­çš„htmlçµæ§‹ 

é€™é‚Šæˆ‘å€‘éœ€è¦å»è§€å¯ŸåŸå§‹ç¶²é ä¸­çš„htmlçµæ§‹ï¼Œæ–¹ä¾¿æˆ‘å€‘äº†è§£è¦æŠ“å–htmlæ¨™ç±¤çš„å“ªå€‹éƒ¨åˆ†ã€‚

åœ¨æ¨™ç±¤div.btn.groupä¸‹
[0]['href'] æ˜¯çœ‹æ¿é€£çµ 
[1]['href'] æ˜¯ç²¾è¯å€é€£çµ
[2]['href'] æ˜¯æœ€èˆŠ 
[3]['href'] æ˜¯ä¸Šé  
[4]['href'] æ˜¯ä¸‹é 
[5]['href'] æ˜¯æœ€æ–°

![](https://hackmd.io/_uploads/rkOTvWh-p.png)


### (è³‡æ–™è™•ç†)ä¾æ“šç•™è¨€æ•¸é‡çš„é †åºä¾†æ’åºæŠ“å–åˆ°çš„æ–‡ç« 

1.æˆ‘å€‘é€™é‚Šè¨­ç½®äº†äºŒç¶­listï¼Œscore[j]æ˜¯ç¬¬jå€‹æ–‡ç« ï¼Œscore[j][0]å­˜æ”¾è“‹æ¨“ç•™è¨€æ•¸é‡ã€‚score[j][1]å­˜æ”¾æ­¤æ–‡ç« é€£çµ
2. å› ç‚ºç•™è¨€æ•¸é‡å­˜æ”¾åœ¨å€‹åˆ¥æ–‡ç« çš„htmlä¸­ï¼Œæ‰€ä»¥å¿…é ˆå†é‡å°æ¯å€‹æ–‡ç« çš„é€£çµåšä¸€æ¬¡è§£æï¼Œç„¶å¾ŒæŠ“å–ç•™è¨€æ•¸é‡çš„æ¨™ç±¤ï¼Œå­˜æ”¾é€²å…¥score[j][0]
3. æœ€å¾Œä½¿ç”¨sort()ï¼Œé‡å°ç•™è¨€æ•¸é‡ï¼ŒæŠŠé€™å€‹äºŒç¶­liståšæ’åºï¼Œé€™æ¨£å°±æœƒå¾—åˆ°ä¾æ“šç•™è¨€æ•¸é‡å¤§å°ä¾†æ’åºçš„æ‰€æœ‰æŠ“å–æ–‡ç« ã€‚

```python=
push = [] #è¨­å®šä¸€å€‹pushçš„ä¸²åˆ— å»å„²å­˜è“‹æ¨“æ•¸
#score[j]æ˜¯ç¬¬jå€‹æ–‡ç« ï¼Œ score[j][0] å’Œscore[j][1] æ˜¯å­˜æ”¾è“‹æ¨“æ•¸é‡ é‚„æœ‰æ­¤ç¶²é çš„link

score = [[0 for j in range(2)] for k in range(300)] #è¨­å®šä¸€å€‹é›™é‡é™£åˆ—å»å„²å­˜æ’åºçš„ç¶²å€å’Œè“‹æ¨“æ•¸ æ–¹ä¾¿å»æ¯”å°
for j in range(len(link)):  #æŠŠè¿´åœˆçš„rangeè¨­å®šæˆæˆ‘å€‘æ“æœ‰çš„ç¶²å€çš„æ•¸é‡  
    response = rs.get(str(link[j]), headers = my_headers) # é‡å°linkä¸­æ¯ä¸€ç¯‡æ–‡ç« ï¼Œå¾å¿ƒå†å»è¨ªå•ä¸€æ¬¡
    soup = bs4.BeautifulSoup(response.text, "html.parser") #å¾—åˆ°æ–‡ç« å…§é çš„åŸå§‹ç¢¼å­˜åˆ°soup
    push = soup.find_all("div", class_ = "push") #æŠŠè“‹æ¨“çš„æ•¸é‡å­˜åˆ°è®Šæ•¸pushä¸­ æ‰€æœ‰pushçš„æ¨™ç±¤éƒ½æœƒè¢«æŠ“å‡ºä¾†ï¼Œæ‰€ä»¥å¯ä»¥ç”¨len(push)çœ‹ç¸½æ•¸é‡
    
    score[j][0] = len(push) #å–å‡ºè“‹æ¨“æ•¸é‡ ä¸¦å­˜å…¥score listçš„0ä½ç½®
    score[j][1] = link[j] #æŠŠlinkå­˜å…¥score listçš„1ä½ç½®
    score.sort(key=lambda x:x[0], reverse =True) #ä½¿ç”¨pythonçš„ä¸²åˆ—æ’åºçš„å‡½å¼sort ï¼Œæ ¹æ“šè“‹æ¨“æ•¸é‡æ’åºå¥½   

```


### å¯«å…¥csvæª”æ¡ˆéƒ¨åˆ† 
1. å› ç‚ºæˆ‘å€‘å‰é¢æ‰€å„²å­˜çš„æ˜¯é€£çµï¼Œæ‰€ä»¥é€™è£¡è¦å†ä½¿ç”¨ bs4.BeautifulSoup è§£æç¶²é åŸå§‹ç¢¼
2. é€™é‚Šæœƒåšä¸€å€‹for loop ä»£è¡¨ä½¿ç”¨è€…æƒ³è¦å‰nç¯‡æ–‡ç« ã€‚
3. ä¸¦ä¸”æŠŠæ–‡å­—å¤šé¤˜çš„éƒ¨åˆ†åšsliceåˆ‡å‰²ï¼Œæœ€å¾Œå­˜å…¥æˆ‘å€‘çš„csvæª”æ¡ˆ
```python=
with open(setting.file_name, "a", newline="", encoding='utf-8-sig')as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['çˆ¬èŸ²çˆ¬å–æ™‚é–“',"æ¨æ–‡æ•¸", "æ¨™é¡Œ","æ–‡ç« é€£çµ","ä½œè€…:", "å…§æ–‡","ç™¼å¸ƒæ™‚é–“"])
    sort_number=10
    for n in range(sort_number): #ç”¨è¿´åœˆå°å‡ºæŠ“å–ä¹‹å‰10å€‹æ¨éœ€æ–‡æ•¸æœ€å¤šçš„æ–‡ç« 
        response = rs.get(score[n][1], headers = my_headers)  # næ˜¯æ–‡ç« æ•¸é‡ï¼Œ[n][1]æ˜¯ç¶²å€é€£çµï¼Œ[n][0] æ˜¯pushæ•¸é‡ã€‚
        soup = bs4.BeautifulSoup (response.text, "html.parser") #å†å°ç¶²ç«™åšä¸€æ¬¡è§£æ
        header = soup.find_all("span", "article-meta-value") #é€™é‚Šget value çš„å®šç¾©æ˜¯æŒ‡æŠŠæ‰€æœ‰çš„article-meta-valueæ¨™ç±¤éƒ½æŠ“ä¸‹ä¾†å­˜æˆlist 

        author = header[0].text #å­˜å…¥ä½œè€…  é€™é‚Šåˆ†åˆ¥ä½¿ç”¨header[]æ˜¯å› ç‚ºä»–åˆ†åˆ¥å­˜åœ¨article-meta-valueä¸‹ 
        title = header[2].text #å­˜å…¥æ¨™é¡Œ
        date = header[3].text #å­˜å…¥æ—¥æœŸ

        main_container = soup.find(id = "main-container",) #æŠ“å‡ºå…§æ–‡
        #print(main_container)
        content = main_container.text.replace("\u6ca1", "  ") 
        #print(content)
   
        all_content = content
        pre_content = all_content.split("--")[0]
        texts = pre_content.split('\n')
        contents = texts[2:]
        final_contents = "\n".join(contents)

        writer.writerow([time_today,score[n][0], title,score[n][1],author, final_contents,date]) #å¯«å…¥csvæª”
        print(score[n][1])
print("æ­å–œå®Œæˆæ–‡ç« çˆ¬èŸ²,è«‹å»result.csvè§€çœ‹çµæœ")
```



ä»¥ä¸Šç‚ºæ­¤ç¨‹å¼ç¢¼çš„ä»‹ç´¹ï¼